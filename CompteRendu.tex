%Classe du document, A4, police taille 12
\documentclass[a4paper,12pt]{article}

% Dictionnaire français, pour caractères spéciaux, tirets, caractères accentués
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
%Toujours plus d'accents
\usepackage[T1]{fontenc}

\usepackage{indentfirst}
%Pour créer des paragraphes random
\usepackage{lipsum}  
%bibliographie
%Le style dépend du projet, voir avec le grand chef
%A mettre à l'endroit où vous voulez la faire apparaitre
%Donc dans le code pas ici t'as vu
%\bibliographystyle{ieeetr}
%\bibligraphy{nom_du_fichier.bib}

%hauteur entre deux lignes
\baselineskip 200cm
%hauteur entre deux paragraphes
\parskip 2mm
%longueur d'indentation
\parindent 2mm
%(on utilise \indent et \noindent sinon)

%Gérer ses marges

%Facilement
\usepackage[margin=2cm]{geometry}

%Précisément
%\usepackage[left=2cm , right=2cm, bottom=2cm, top=2cm, headheight=2cm]{geometry} 
%header c'est l'en-tête pas la marge supérieure

%Toujours plus précisément
%\addtolength{\oddsidemargin}{-0.5in}
%\addtolength{\evensidemargin}{-5cm}
%\addtolength{\topmargin}{-0.5in}

%Faires des articles  plusieures colonnes
\usepackage{multicol}
%Separation des colonnes
\setlength{\columnsep}{2cm}

%Avoir des entêtes et pieds de page stylés
\usepackage{fancyhdr}
\pagestyle{fancy}
%Pour enlever l'entête avec les sections
%\fancyhf{}

%Ca se fait sous format \<pos><type>{<contenu>}
%type c'est "head" ou "foot"
%pos pour position gauche "l", droite "r" ou centre "c"
%contenu c'est ce que tu mets dans dedans 
%marche aussi avec des images mais flemme
%mettre un trait
%\renewcommand{\footrulewidth}{1.5pt}

% Liens dans le document
\usepackage{hyperref}  
% Légendes dans les environnements "figure" et "float"
\usepackage{subcaption}
%La base pour faire des figures juste
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{wrapfig}
%Trucs utiles pour les maths
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{definition}{Définition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemme}
\newtheorem{theorem}{Théorème}
\newtheorem{remark}{Remarque}

\usepackage{listings}


\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{0.5cm}
        {\Large ÉCOLE NATIONALE DES PONTS ET CHAUSSÉES}\\
        \vspace{1cm}
        \rule\linewidth{0.05cm}
        {\huge Résolution d'une équation aux dérivées partielles par processus gaussien\par}
        \rule\linewidth{0.05cm}
        \vspace{1cm}
        {\Large Projet 1A\par}
        \vspace{0.8cm}
        {\Large Par}\\
        \vspace{0.3cm}
        {\large \textit{Nils Baulier, Raphaël Viard, Salma Kably et Simon Weissberg}}\\
        \vspace{0.8cm}
        {\Large Supervisé par}\\
        \vspace{0.3cm}
        {\large \textit{Urbain Vaes}}\\
        \vspace{0.7cm}
        \includegraphics[scale=0.2]{logo_ponts.jpg}\\
    \end{center}
\end{titlepage}

\tableofcontents

\section{Introduction}

Historiquement, les processus de régression gaussienne furent introduits par Krige en 1960,
 cherchant à l’époque à obtenir la distribution d’or dans un milieu, en ne se basant que 
 sur des échantillons. Aujourd’hui, ils permettent par exemple d’étudier la perméabilité 
 radioactive d’un sol. L'approche de résolution d'équations aux dérivées partielles par 
 processus gaussien est une technique d'approximation de la solution par une fonction ayant
 un caractère aléatoire, permettant par exemple de modéliser l'incertitude sur une donnée du problème. 
 Avant de se lancer dans une telle résolution, il parait naturel d'abord de simplement essayer 
 d'approcher une fonction connue par un processus gaussien (c'est la régression gaussienne), 
 ce qui sera l'objet principal de ce premier rendu. Cette première étape nous orientera pour 
 ensuite résoudre des équations différentielles linéaires.

\section{Processus gaussien}

\subsection{Définition}

\begin{definition}
    Un processus stochastique est une famille de variables aléatoires $({X_t})_{t \in T}$ 
    définies sur le même espace de probabilité $(\Omega,\mathcal{F},\mathcal{P})$ indexée par 
    $T$ et à valeurs dans $S$. Un processus stochastique peut donc aussi être vu comme une application 
    $X:\Omega \times T \to S$ tel que 
    pour tout $t$ dans $T$, $X( . ,t)$ est une variable aléatoire.

    Un processus gaussien est un processus stochastique $({X_t})_{t \in T}$ tel que 
    $\forall n \in \mathbb{N}, \forall (t_1,t_2,...,t_n) \in T^{n}, (X_{t_1},...,X_{t_n})$
     est un vecteur gaussien.
\end{definition}

\begin{proposition}
    Soit $({X_t})_{t \in T}$ un processus gaussien. Alors il existe $\bar{m}:T \to \mathbb{R}$ et
     $\bar{k}:T \times T \to \mathbb{R}$ définie positive tel que $\forall n \in \mathbb{N},
      \forall (t_1,t_2,...,t_n) \in T^{n}, (X_{t_1},...,X_{t_n}) 
      \sim \mathcal{N}( (\bar{m}(t_1) , ... , \bar{m}(t_n))^\top , K_{t_1,t_2,...,t_n} )$ 
      où $K_{t_1,t_2,...,t_n}$ est la matrice de coefficients $K_{i,j}=\bar{k}(t_i,t_j)$.
    
    Réciproquement, s'il existe un tel m et un tel c, alors $({X_t})_{t \in T}$ est un processus gaussien. On note alors $({X_t})_{t \in T} \sim \mathcal{GP}(\bar{m},\bar{k})$ et on appelle $\bar{m}$ la fonction moyenne et $\bar{k}$ le noyau de covariance du processus gaussien.
\end{proposition}

\subsection{Modélisation d'une trajectoire d'un processus gaussien}

\section{Interpolation gaussienne}

\subsection{Régression gaussienne}

\begin{definition}
    Soit $\chi$ un ensemble non vide et $f:\chi\to\mathbb{R}$ une fonction.
    Soit $n\in\mathbb{N}$ et $(x_i,y_i)_{i=1}^n\subset\chi\times\mathbb{R}$ tel que
    $$ ~~ y_i=f(x_i)+\xi_i,~~~~~~~~i=1,...n$$ 
    Les $(x_i,y_i)$ sont les points que l'on cherche à interpoler
    et les $\xi_i$ sont des variables aléatoires qui représentent le "bruit", 
    ou alors l'incertitude/variabilité des réponses que l'on peut avoir pour une même entrée.

    Le problème de la régression est d'estimer la fonction f en se basant $(*)$. 
    La fonction $f$ est alors appelée fonction de régression.

    La régression gaussienne est d'imposer $f\sim\mathcal{GP}(m,k)$ et $\xi_i\sim\mathcal{N}(0,\sigma^2),~~~~i=1,...,n$. 
    Ici, $m$ et $k$ doivent être choisis de manière à représenter des connaissances 
    ou des hypothèses sur la fonction de régression.
\end{definition}

\begin{lemma}
    Soit $a\in\mathbb{R}^d$ et $b\in\mathbb{R}$ des vecteurs gaussiens tels que 
$$
\begin{bmatrix} a \\ b \end{bmatrix} \sim \mathcal{N} \begin{pmatrix} \begin{bmatrix} \mu_a \\ \mu_b \end{bmatrix} \ \begin{bmatrix} A \ C \\ C^T \ B \end{bmatrix} \end{pmatrix}
$$
où $\mu_a$ et $\mu_b$ sont les vecteurs moyennes, $A$ et $B$ les matrices de covariances et $C\in\mathbb{R}^{n\times m}$.
Alors la distribution conditionnelle de b sachant a est donnée par
$$ b|a \sim \mathcal{N}(\mu_b+C^TA^{-1}(a-\mu_a),B-C^TA^{-1}C) $$
\end{lemma}

\begin{theorem}
    Supposons $f$ solution du problème de régression gaussienne. Alors
$$ f|Y\sim\mathcal{GP}(\bar{m},\bar{k}),
$$
où $\bar{m}:\chi\to\mathbb{R}$ et $\bar{k}:\chi\times\chi\to\mathbb{R}$ sont données par
$$ \bar{m}(x)=m(x)+k_{xX}(k_{XX}+\sigma^2I_n)^{-1}(Y-m_X),~~~~~~~~~~x\in\chi $$
$$ \bar{k}(x,x')=k(x,x')-k_{xX}(k_{XX}+\sigma^2I_n)^{-1}k_{Xx'}, ~~~~~ x, x'\in\chi$$
où $k_{Xx}=k_{xX}^{T}=(k(x_1,x),...,k(x_n,x))^{T}$ et $m_X^{T}=(m(x_1),...m(x_n))^{T}$
\end{theorem}

\subsection{Espace à noyau reproduisant}

Une façon de retrouver l'expression de la moyenne obtenu dans le Théorème 1 est
de d'utiliser un espace à noyau reproduisant.

\begin{definition}
    Soit $\chi$ un ensemble non vide. Un noyau défini positif est une fonction symétrique $k : \chi \times \chi \to \mathbb{R}$ tel que $\forall n \in \mathbb{N}, (c_1,...,c_n) \in \mathbb{R}^n et (x_1,...,x_n) \in \chi^n$
$$
\sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i,x_j) \ge 0.
$$
\end{definition}

\begin{remark}
    En reprenant les notations de I.a., on observe que le noyau de covariance c peut 
    être vu comme un noyau défini positif sur $T \times T$. 
\end{remark}

\begin{definition}
    Soit $\chi$ un ensemble non vide. Un espace de Hilbert $(\mathcal{H}_k,\langle .,.\rangle_{\mathcal{H}_k})$ de fonctions de $\chi$ est appelé un espace à noyau reproduisant de noyau reproduisant $k$ si les conditions suivantes sont satisfaites :
$$
(i) \forall x\in\chi, k(.,x)\in\mathcal{H}_k;
$$
$$
(ii) \forall x\in\chi,\forall f\in\mathcal{H}_k,f(x)=\langle f,k(.,x)\rangle_{\mathcal{H}_k}
$$
\end{definition}

\begin{remark}
    $(ii)$ équivaut à dire que l'évaluation est continue sur l'espace des fonctions de $\mathcal{H}_k$
\end{remark}

\subsection{Méthode des moindres carrés}

Nous souhaitons interpoler l'ensemble de points ${(x_1,y_1),...,(x_n,y_n)}$.
Nous nous intéressons d'abord à résoudre le problème 
$$
\hat{f} = argmin(\frac{1}{n}\sum_{i=1}^nL(x_i,y_i,f(x_i))+\lambda\lVert f\rVert_{\mathcal{H}_k}^2 | f\in\mathcal{H}_k)
$$
où $L:\chi\times\mathbb{R}\times\mathbb{R}\to\mathbb{R}^{+}$ est une fonction de perte et $\lambda >0$ une constante de régularisation. L joue sur la différence entre les points $y_i$ et les $f(x_i)$. Nous allons ici nous intéresser à l'estimateur des moindres carrés $L(x,y,y')=(y-y')^2$ donc au problème suivant :
$$
\hat{f} = argmin(\frac{1}{n}\sum_{i=1}^n(f(x_i)-y_i)^2+\lambda\lVert f\rVert_{\mathcal{H}_k}^2 | f\in\mathcal{H}_k)
$$

\begin{theorem}
    La solution est unique, et est donnée par
$$
\hat{f}(x)=k_{xX}(k_{XX}+n\lambda I_{n})^{-1}Y
$$
\end{theorem}

\begin{proof}
Notons $h:f\in\mathcal{H}_k\to\mathbb{R}$ la fonction $h(f)=\frac{1}{n}\sum_{i=1}^n(f(x_i)-y_i)^2+\lambda\lVert f\rVert_{\mathcal{H}_k}^2$, qu'on cherche donc à minimiser.

Posons $\mathcal{H}_0=vect(\{k(.,x_i)\}_{i=1,...,n})$. $\mathcal{H}_0$ est un sous espace vectoriel de $\mathcal{H}_k$. Ainsi, on peut considérer $\mathcal{H}_0^\perp$ supplémentaire orthogonal de $\mathcal{H}_0$. 

Soit $f^*$ solution optimale du problème. Alors il existe $f^*_0\in\mathcal{H}_0$ et $g^*\in\mathcal{H}_0^\perp$ tel que $f^*=f^*_0+g^*$

Ainsi, $\lVert f^*\rVert_{\mathcal{H}_k}^2=\lVert f^*_0\rVert_{\mathcal{H}_k}^2+\lVert g^*\rVert_{\mathcal{H}_k}^2$ (théorême de pythagore)

et $f^*(x_i)=\langle f,k(.,x_i)\rangle_{\mathcal{H}_k}=\langle f^*_0,k(.,x_i)\rangle_{\mathcal{H}_k}+\langle g^*,k(.,x_i)\rangle_{\mathcal{H}_k}=\langle f^*_0,k(.,x_i)\rangle_{\mathcal{H}_k}$ car $g^*\mathcal{H}_0^\perp$

donc finalement $h(f^*)=h(f^*_0)+\lVert g^*\rVert_{\mathcal{H}_k}^2$ où $\lVert g^*\rVert_{\mathcal{H}_k}^2 >0$ si $f^*$ n'appartient pas à $\mathcal{H}_0$, ce qui montre que $f^*_0$ est une meilleure solution au problème, ce qui n'est pas car f est par hypothèse la solution optimale de ce dernier. Donc la solution du problème si elle existe est un élément de $\mathcal{H}_0$.

Il suffit alors de résoudre le problème sur $\mathcal{H}_0$ (espace de dimension finie), ce qui aboutit à la formule annoncée.
\end{proof}

\begin{proposition}
    Soit $k$ un noyau défini positif sur un ensemble $\chi$ et définissons $X:=(x_1,...,x_n)\in\chi^n$ et $Y:=(y_1,...,y_n)\in\mathbb{R}^n$. Alors $\bar{m}=\hat{f}$ si $\sigma^2=n\lambda$ où

$\bar{m}$ est la fonction moyenne de la regression gaussienne sur $(X,Y)$, le prayeur du processus gaussien $\mathcal{f}\sim\mathcal{GP}(0,k)$ en considérant le bruit $\xi_1,...,\xi_n\sim\mathcal{N}(0,\sigma^2)$

$\hat{f}$ est la solution de la régression des moindres carrées de $(X,Y)$ basé sur l'espace à noyau reproduisant associé à $k$.
\end{proposition}

\subsection{Interpolation}

\begin{minted}{julia}
    function interpolation_gaussienne_discret(T,X,Y,k,m,n)
        M=zeros(length(T))
        MX=m.(X')
        KXX=k.(X,X')
        A=zeros(length(T),length(T))
        for i=1:length(T)
            J=zeros(length(X))
            for l=1:length(X)
                J[l]=(k(T[i],X[l]))
            end
            M[i]=m(T[i])+( J'*(inv(KXX+sigma^2*Matrix(I,length(X),length(X)))*(Y'-MX) ))[1]
            for j=1:length(T)
                K=zeros(length(X))
                for l=1:length(X)
                    K[l]=k(X[l],T[j])
                end
                A[i,j]=k(T[i],T[j])-
                ( J'*(inv(KXX+sigma^2*Matrix(I,length(X),length(X)))*K))[1]
            end
        end
        B=zeros(length(T))
        for i=1:length(T)
            B[i]=A[i,i]
        end
        sqrtA=real(sqrt(A))
        Plots.plot(title="Régression gaussienne")
        for i=1:n
            LN=randn(size(T))
            Y=M+sqrtA*LN
            Plots.plot!(T,Y)
        end
        Plots.plot!(T,M,ribbon=4*B,label="Moyenne",linewidth=3)
    end
         
    
    function interpolation_gaussienne(T,X,Y,k,m,n)
        k_XX=k.(X,X')
        m_X=m.(X')
        M(x)=m(x)+( k.(x,X')'*inv(k_XX+sigma^2*Matrix(I,length(X),length(X)))*
        (Y'-m_X))[1] # Moyenne de f conditionné sur Y
        K(x,y)=k(x,y)-( k.(x,X')'*(inv(k_XX+sigma^2*Matrix(I,length(X),length(X)))*
        k.(X,y)'))[1] # Covariance de f conditionné sur Y
        Plots.plot(title="Régression gaussienne")
        M_X=M.(T')
        K_XX=K.(T,T')
        R=real(sqrt(K_XX))
        C=zeros(length(T))
        for i=1:length(T)
            C[i]=K_XX[i,i]
        end
        for i=1:n
            u=randn(size(T))
            f_X=M_X'+R*u
            Plots.plot!(T,f_X)
        end
        Plots.plot!(T,M_X',ribbon=4*C,label="Moyenne",linewidth=3)
    end
    
    
    T2=-π:0.01:π
    X2=[-2.5 -1 0.1 0.7 0.9 1]
    Y2=[1 0 2 0.3 0.5 -1]
    
    interpolation_gaussienne(T2,X2,Y2,c,m,10)
    \end{minted}

\section{Equations aux dérivées partielles linéaires}

\subsection{Définition}

\begin{definition}
    Soit $\mathcal{L}:\mathbb{U}\to\mathbb{U}$ un opérateur linéaire.
    On appelle équation linéaire sur $\mathbb{U}$ une équation de la forme
    $\mathcal{L}[u] = f$
\end{definition}

\begin{proposition}
    Les equations aux dérivéees partielles sont des équations linéaires sur
    $\mathbb{U}$ où ici $\mathbb{U}$ est un espace de fonctions suffisamment
    régulière pour que l'opérateur différentiel linéaire considéré y soit 
    défini.
\end{proposition}

\subsection{Conditionner sur un opérateur linéaire}

\begin{theorem}
    Soit $u\sim\mathcal{GP}(m,k)$, $\mathcal{L}:\mathbb{U}\to\mathbb{R}$ un opérateur linéaire et une variable gaussienne indépendante $\epsilon \sim \mathcal{N}(\mu,\Sigma)$. Alors 
$$
u | (\mathcal{L}[u]+\epsilon=y)\sim\mathcal{GP}(m^{u|y},k^{u|y})
$$
où la moyenne et la covariance sont données par 
$$
m^{u|y}(x)=m(x)+\mathcal{L}[k(.,x)]^{T}(\mathcal{L}k\mathcal{L}+\Sigma)^{-1}(y-(\mathcal{L}[m]+\mu))
$$
$$
k^{u|y}=k(x_1,x_2)+\mathcal{L}[k(.,x_1)]^{T}(\mathcal{L}k\mathcal{L}+\Sigma)^{-1}\mathcal{L}[k(.,x_2)]
$$
où $(\mathcal{L}k\mathcal{L})_{ij}:=\mathcal{L}[x\mapsto\mathcal{L}[k(x,.)_j]_i$
\end{theorem}

\subsection{Convergence}

Il est judicieux de prouver que notre solution approchée de l'EDP converge vers la solution réelle.

\subsection{Equation de Poisson}

\subsection{Equation d'advection diffusion}

\section{Equation aux dérivées partielles non linéaires}

Soit d $\geq$ 1 et $\Omega$ un ouvert borné de $\mathbf{R^{d}}$ 

Considérons l'équation non linéaire vérifiée par $u^{*}$:
\begin{equation}
\label{eq:system}
\left
\begin{aligned}
\Delta u^{*}(x)+ \tau u^{*}(x)=f(x), \qquad \forall x \in \Omega \\
 u^{*}(x) = g(x), \qquad x \in \partial \Omega
\end{aligned}
\right
\end{equation}


avec   $ f : \Omega \xrightarrow{} \mathbf{R}$ ,
   $ g :  \partial \Omega \xrightarrow{} \mathbf{R}$   et $ \tau : \mathbf{R} \xrightarrow{} \mathbf{R}$ \quad  sont des fonctions continues données. On suppose que  $ f, g $ et  $ \tau $ sont choisis de manière à ce que l'EDP ait une solution unique. \\
   

On considère un noyau symétrique, défini positif \quad $K :\Omega \times \Omega \xrightarrow{} \Omega $ \quad avec $ \Omega := \Omega \cup \partial \Omega  . $  Soit $ U $ le RKHS  associé à K et $\|.\|$, la norme associée au RKHS. 
Soit $ 1 \leq M_\Omega \leq M \leq \infty $ et fixons M points dans $\Omega$ tel que  $x_1, ..., x_{M_\Omega} \in \Omega$ et  $x_{M_\Omega + 1}, ...,x_M \in \partial\Omega $ La méthode suivante approche la solution $u^{*}$ de \eqref{eq:system}  avec \u^{\textdagger} un minimiseur du problème d'optimisation suivant: 


\subsection{Equation de Fischer}

\end{document}
